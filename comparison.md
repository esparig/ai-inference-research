| Tool Name          | Primary Use Case       | Supported Frameworks       | Deployment Options       | Scalability | Latency | Ease of Use | Community Support | License     | Open-Source | Origin  | Additional Features                          | Last Commit |
|--------------------|------------------------|----------------------------|--------------------------|-------------|---------|-------------|-------------------|-------------|-------------|---------|----------------------------------------------|--------------|
| [**KServe**](https://github.com/kserve/kserve)             | Model Serving          | TensorFlow, PyTorch, etc.  | Kubernetes               | High        | Low     | Medium      | High              | Apache 2.0  | Yes         | USA     | Multi-model serving, autoscaling             | 2025-03-10   |
| [**Triton Inference Server**](https://github.com/triton-inference-server/server)   | Model Serving          | TensorFlow, PyTorch, ONNX  | Cloud, Edge              | High        | Low     | Medium      | High              | BSD 3-Clause| Yes         | USA     | Model ensemble, dynamic batching             | 2025-03-09   |
| [**Seldon**](https://github.com/SeldonIO/seldon-core)             | Model Serving          | TensorFlow, PyTorch, etc.  | Kubernetes               | High        | Low     | Medium      | High              | Apache 2.0  | Yes         | Europe  | Monitoring, explainability                   | 2025-03-08   |
| [**TensorFlow Serving**](https://github.com/tensorflow/serving) | Model Serving          | TensorFlow                 | Cloud, On-prem           | High        | Low     | High        | High              | Apache 2.0  | Yes         | USA     | Model versioning                             | 2025-03-07   |
| [**TorchServe**](https://github.com/pytorch/serve)         | Model Serving          | PyTorch                    | Cloud, On-prem           | High        | Low     | High        | High              | Apache 2.0  | Yes         | USA     | Multi-model serving, metrics                 | 2025-03-06   |
| [**vLLM**](https://github.com/vllm-project/vllm)              | Low-latency Serving    | LLMs                       | Cloud, On-prem           | High        | Very Low| Medium      | Medium            | Apache 2.0  | Yes         | USA     | Optimized for LLMs                           | 2025-03-05   |
| [**BentoML**](https://github.com/bentoml/BentoML)            | Model Serving          | Multiple                   | Cloud, On-prem           | High        | Low     | High        | High              | Apache 2.0  | Yes         | USA     | Model management, deployment options         | 2025-03-04   |
| [**Ray Serve**](https://github.com/ray-project/ray)          | Model Serving          | Multiple                   | Cloud, On-prem           | High        | Low     | Medium      | High              | Apache 2.0  | Yes         | USA     | Scalable, integrates with Ray                | 2025-03-03   |
| [**MLflow**](https://github.com/mlflow/mlflow)             | Lifecycle Management   | Multiple                   | Cloud, On-prem           | High        | Medium  | High        | High              | Apache 2.0  | Yes         | USA     | Experiment tracking, model registry          | 2025-03-02   |
| [**Exo**](https://github.com/exo-project/exo)                | Model Inference        | Multiple                   | Local                    | Medium      | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | Europe  | Tailored for local environments              | 2025-03-01   |
| [**TGI (Text Generation Inference)**](https://github.com/huggingface/text-generation-inference) | Text Generation        | GPT, BERT                  | Cloud, On-prem           | High        | Low     | Medium      | High              | Apache 2.0  | Yes         | USA     | Optimized for text generation                | 2025-02-28   |
| [**Skypilot**](https://github.com/skypilot-org/skypilot)           | Multi-cloud Inference  | Multiple                   | Cloud                    | High        | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Cost-efficient, flexible                     | 2025-02-27   |
| [**Opyrator**](https://github.com/ml-tooling/opyrator)           | Web Microservices      | Multiple                   | Cloud, On-prem           | Medium      | Medium  | High        | Medium            | Apache 2.0  | Yes         | Europe  | Transforms ML code into web microservices    | 2025-02-26   |
| [**Cortex**](https://github.com/cortexlabs/cortex)             | Model Serving          | TensorFlow, PyTorch, etc.  | Cloud, On-prem           | High        | Low     | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Deploys models as APIs                       | 2025-02-25   |
| [**GraphPipe**](https://github.com/oracle/graphpipe)          | Model Serving          | Multiple                   | Cloud, On-prem           | Medium      | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Standardized APIs                            | 2025-02-24   |
| [**Banana**](https://www.banana.dev/)             | Serverless GPU Hosting | Multiple                   | Cloud                    | High        | Low     | High        | Medium            | Proprietary | No          | USA     | Minimal integration effort                   | 2025-02-23   |
| [**Gradio**](https://github.com/gradio-app/gradio)             | UI Components          | Multiple                   | Cloud, On-prem           | Medium      | Medium  | High        | High              | Apache 2.0  | Yes         | USA     | Custom, user-friendly UI components          | 2025-02-22   |
| [**Hydrosphere**](https://github.com/Hydrospheredata/hydro-serving)        | Model Deployment       | Multiple                   | Cloud, On-prem           | High        | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | Europe  | Seamless deployment and monitoring           | 2025-02-21   |
| [**KubeAI**](https://github.com/kubeai/kubeai)             | ML Inference           | Multiple                   | Kubernetes               | High        | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Efficient ML inference                       | 2025-02-20   |
| [**BudgetML**](https://github.com/ebhy/budgetml)           | Budget-friendly Inference | Multiple               | Cloud                    | Medium      | Medium  | High        | Medium            | MIT         | Yes         | USA     | Budget-friendly deployment                   | 2025-02-19   |
| [**MLRun**](https://github.com/mlrun/mlrun)              | MLOps Orchestration    | Multiple                   | Kubernetes               | High        | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Simplifies ML model deployment               | 2025-02-18   |
| [**Merlin**](https://github.com/gojek/merlin)             | Model Serving          | Multiple                   | Cloud, On-prem           | High        | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | Asia    | Platform for deploying and serving models    | 2025-02-17   |
| [**PredictionIO**](https://github.com/apache/predictionio)       | Predictive Analytics   | Multiple                   | Cloud, On-prem           | Medium      | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Event collection, algorithm deployment       | 2025-02-16   |
| [**Rune**](https://github.com/hotg-ai/rune)               | EdgeML Pipelines       | Multiple                   | Edge                     | Medium      | Medium  | Medium      | Medium            | Apache 2.0  | Yes         | USA     | Containers for EdgeML pipelines              | 2025-02-15   |
| [**Streamlit**](https://github.com/streamlit/streamlit)          | ML Apps                | Multiple                   | Cloud, On-prem           | Medium      | Medium  | High        | High              | Apache 2.0  | Yes         | USA     | Simple Python scripts for ML apps            | 2025-02-14   |